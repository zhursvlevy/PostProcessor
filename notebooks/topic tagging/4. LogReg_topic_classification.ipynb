{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression for topic classification\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..\\..')) # Path to root folder\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path + \"/scripts\") # define scripts path\n",
    "\n",
    "from ipynb_func import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NUM = 10 # Number of data parquets to use\n",
    "#assert NUM >= 1 and NUM <= 10, \"NUM value must be in range [1, 10]\"\n",
    "\n",
    "# Making list of roots to merge processed raw data \n",
    "#paths = [module_path + f\"/data/pikabu/tag_processed/raw_data/{i}_tag_processed.parquet\" for i in range(NUM)] \n",
    "\n",
    "# Making list of roots to merge processed filtered data\n",
    "#paths = [module_path + f\"/data/pikabu/tag_processed/filtered_data/{i}_tag_processed.parquet\" for i in range(NUM)] \n",
    "\n",
    "# Making list of roots to merge processed cleared data\n",
    "paths = [module_path + f\"/data/pikabu/splited_data/cleared_texts.parquet\"] \n",
    "\n",
    "data = merge_dataset(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_markdown</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6991359</td>\n",
       "      <td>[добрый, сутки, господин, дама, подсказывать, название, игра, телефон, оформление, убийство, зомби, очки, ездить, машинка, крутить, развивать, скорость, заранее, благодарить]</td>\n",
       "      <td>[игры, поиск]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>7004423</td>\n",
       "      <td>[ехать, девчонка, школа, оставаться, свободный, макс, заявка, прямой, конечный, адрес, железнодорожный, институт, включать, вбивать, адрес, выдавать, столовая, ладно, садиться,...</td>\n",
       "      <td>[юмор]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>6991603</td>\n",
       "      <td>[стадо, стадо, гигантский, случаться, стадо, управлять, волк, предел, волк, жопа, враг, дружно, осматривать, выдавливать, стадо, выдавливать, съедать, волк, близкий, холм, обхо...</td>\n",
       "      <td>[мат]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  \\\n",
       "15  6991359   \n",
       "37  7004423   \n",
       "52  6991603   \n",
       "\n",
       "                                                                                                                                                                          text_markdown  \\\n",
       "15       [добрый, сутки, господин, дама, подсказывать, название, игра, телефон, оформление, убийство, зомби, очки, ездить, машинка, крутить, развивать, скорость, заранее, благодарить]   \n",
       "37  [ехать, девчонка, школа, оставаться, свободный, макс, заявка, прямой, конечный, адрес, железнодорожный, институт, включать, вбивать, адрес, выдавать, столовая, ладно, садиться,...   \n",
       "52  [стадо, стадо, гигантский, случаться, стадо, управлять, волк, предел, волк, жопа, враг, дружно, осматривать, выдавливать, стадо, выдавливать, съедать, волк, близкий, холм, обхо...   \n",
       "\n",
       "             tags  \n",
       "15  [игры, поиск]  \n",
       "37         [юмор]  \n",
       "52          [мат]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 180)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Data preparation and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(module_path + f\"/data/pikabu/splited_data/indexes.json\") as f:\n",
    "    id_splits = f.read()\n",
    "\n",
    "id_splits = json.loads(id_splits)\n",
    "\n",
    "data_train = data[data['id'].isin(id_splits['train'])]\n",
    "data_val = data[data['id'].isin(id_splits['val'])]\n",
    "data_test = data[data['id'].isin(id_splits['test'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train data: 25209\n",
      "Number of val data: 2821\n",
      "Number of test data: 3119\n",
      "Distribution: 81 / 9 / 10\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of train data: {len(data_train)}\")\n",
    "print(f\"Number of val data: {len(data_val)}\")\n",
    "print(f\"Number of test data: {len(data_test)}\")\n",
    "print(f\"Distribution: {len(data_train)/len(data)*100:.0f} / {len(data_val)/len(data)*100:.0f} / {len(data_test)/len(data)*100:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\decique\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "Vec = CountVectorizer(tokenizer=lambda x: x.split(','), binary=True)\n",
    "\n",
    "df = data.copy()\n",
    "df.tags = [','.join(i) for i in df.tags]\n",
    "\n",
    "df_train = data_train.copy()\n",
    "df_train.tags = [','.join(i) for i in df_train.tags]\n",
    "\n",
    "df_val = data_val.copy()\n",
    "df_val.tags = [','.join(i) for i in df_val.tags]\n",
    "\n",
    "df_test = data_test.copy()\n",
    "df_test.tags = [','.join(i) for i in df_test.tags]\n",
    "\n",
    "y_data = Vec.fit(df['tags'])\n",
    "y_train = Vec.transform(df_train['tags'])\n",
    "y_val = Vec.transform(df_val['tags'])\n",
    "y_test = Vec.transform(df_test['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags to predict:\n",
      "['авто' 'авторский рассказ' 'алкоголь' 'анекдот' 'армия' 'вопрос' 'врачи'\n",
      " 'девушки' 'деньги' 'дети' 'детство' 'другое' 'жизнь' 'игры' 'интересное'\n",
      " 'истории' 'история' 'ищу книгу' 'ищу фильм' 'карантин' 'книги'\n",
      " 'коронавирус' 'кот' 'лига добра' 'лига юристов' 'любовь' 'люди' 'мат'\n",
      " 'медицина' 'москва' 'музыка' 'мысли' 'негатив' 'новости' 'новый год'\n",
      " 'общество' 'отношения' 'поиск' 'политика' 'помогите найти' 'помощь'\n",
      " 'психология' 'работа' 'рассказ' 'реальная история из жизни'\n",
      " 'родители и дети' 'россия' 'самоизоляция' 'санкт-петербург' 'семья'\n",
      " 'случай из жизни' 'совет' 'сон' 'соседи' 'стихи' 'украина' 'фантастика'\n",
      " 'фильмы' 'школа' 'юмор']\n"
     ]
    }
   ],
   "source": [
    "print('Tags to predict:')\n",
    "print(Vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_distr = getworddict(getwordlist(data.tags))\n",
    "tag_distr_formated = {}\n",
    "for i in range(len(tag_distr)):\n",
    "    tag_distr_formated[i] = round(tag_distr[Vec.get_feature_names_out()[i]] / sum(tag_distr.values()), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags weights:\n",
      "{0: 0.0073, 1: 0.0121, 2: 0.0075, 3: 0.0079, 4: 0.0085, 5: 0.0196, 6: 0.0077, 7: 0.0138, 8: 0.0089, 9: 0.0279, 10: 0.011, 11: 0.0221, 12: 0.0186, 13: 0.0122, 14: 0.0122, 15: 0.0086, 16: 0.0338, 17: 0.0067, 18: 0.008, 19: 0.014, 20: 0.0107, 21: 0.0454, 22: 0.0121, 23: 0.0142, 24: 0.0078, 25: 0.0149, 26: 0.0084, 27: 0.0549, 28: 0.0125, 29: 0.0098, 30: 0.0081, 31: 0.0093, 32: 0.0145, 33: 0.0152, 34: 0.0133, 35: 0.0237, 36: 0.0206, 37: 0.0071, 38: 0.0308, 39: 0.0118, 40: 0.054, 41: 0.0121, 42: 0.0297, 43: 0.0303, 44: 0.0315, 45: 0.0082, 46: 0.0277, 47: 0.0063, 48: 0.0075, 49: 0.0145, 50: 0.0122, 51: 0.0078, 52: 0.0067, 53: 0.0086, 54: 0.0274, 55: 0.0275, 56: 0.0111, 57: 0.0104, 58: 0.0144, 59: 0.0384}\n"
     ]
    }
   ],
   "source": [
    "print('Tags weights:')\n",
    "print(tag_distr_formated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y shapes:\n",
      "  • Y train: (25209, 60)\n",
      "  • Y validation: (2821, 60)\n",
      "  • Y test: (3119, 60)\n"
     ]
    }
   ],
   "source": [
    "print('Y shapes:')\n",
    "print(f'  • Y train: {y_train.shape}')\n",
    "print(f'  • Y validation: {y_val.shape}')\n",
    "print(f'  • Y test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Training with bag-of-words embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models_path = module_path + '/models/logreg/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = [' '.join(txt) for txt in data.text_markdown]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [' '.join(txt) for txt in data_train.text_markdown]\n",
    "X_val = [' '.join(txt) for txt in data_val.text_markdown]\n",
    "X_test = [' '.join(txt) for txt in data_test.text_markdown]\n",
    "\n",
    "X_Vec = CountVectorizer(tokenizer = lambda x: x.split())\n",
    "\n",
    "X_Vec.fit(X_train)\n",
    "X_train = X_Vec.transform(X_train)\n",
    "X_test = X_Vec.transform(X_test)\n",
    "X_val = X_Vec.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X BoW's shapes:\n",
      "   • X train shape: (25209, 5899)\n",
      "   • X val shape: (2821, 5899)\n",
      "   • X test shape: (3119, 5899)\n"
     ]
    }
   ],
   "source": [
    "print(\"X BoW's shapes:\")\n",
    "print(f'   • X train shape: {X_train.shape}')\n",
    "print(f'   • X val shape: {X_val.shape}')\n",
    "print(f'   • X test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "# Searching for best model params; too long;\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "LogReg_cfg = {'estimator__C':[1e3, 1e5, 1e7, 1e8],\n",
    "              'estimator__penalty': ['elasticnet', 'l1', 'l2'],\n",
    "              'estimator__dual': [False],\n",
    "              'estimator__class_weight': [None, tag_distr_formated],\n",
    "              'estimator__solver': ['lbfgs', 'liblinear', 'newton-cg'],\n",
    "              'estimator__random_state': [42]}\n",
    "\n",
    "clf_ovr = OneVsRestClassifier(estimator=LogisticRegression(),\n",
    "                              n_jobs=-1)\n",
    "\n",
    "GSCV_clf = GridSearchCV(estimator=clf_ovr, param_grid=LogReg_cfg)\n",
    "\n",
    "GSCV_clf.fit(X_train, y_train)\n",
    "\n",
    "GSCV_clf.best_params_ \n",
    "\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg_cfg = {'C':5e7,\n",
    "              'penalty': 'l2',\n",
    "              'dual': False,\n",
    "              'class_weight': tag_distr_formated,\n",
    "              'solver': 'liblinear',\n",
    "              'random_state': 42}\n",
    "\n",
    "clf_ovr = OneVsRestClassifier(estimator=LogisticRegression(C=LogReg_cfg['C'],\n",
    "                                                           dual=LogReg_cfg['dual'],\n",
    "                                                           class_weight=LogReg_cfg['class_weight'],\n",
    "                                                           penalty=LogReg_cfg['penalty'],\n",
    "                                                           solver=LogReg_cfg['solver'],\n",
    "                                                           random_state=LogReg_cfg['random_state']),\n",
    "                              n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(save_models_path + 'bow.joblib'):\n",
    "    clf_ovr = load(save_models_path + 'bow.joblib')\n",
    "else:\n",
    "    clf_ovr.fit(X_train, y_train)\n",
    "    dump(clf_ovr, save_models_path + 'bow.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val = clf_ovr.predict(X_val)\n",
    "\n",
    "df_val = data_val.copy()\n",
    "df_val['predicted_tags'] = Vec.inverse_transform(y_pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Bag-of-Words:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.28      0.30        25\n",
      "           1       0.26      0.22      0.24        45\n",
      "           2       0.48      0.30      0.37        40\n",
      "           3       0.23      0.26      0.24        27\n",
      "           4       0.46      0.44      0.45        27\n",
      "           5       0.09      0.13      0.10        68\n",
      "           6       0.43      0.33      0.37        40\n",
      "           7       0.23      0.27      0.25        59\n",
      "           8       0.10      0.10      0.10        42\n",
      "           9       0.24      0.32      0.28       113\n",
      "          10       0.25      0.26      0.26        50\n",
      "          11       0.04      0.05      0.05        85\n",
      "          12       0.06      0.10      0.08        72\n",
      "          13       0.51      0.52      0.51        58\n",
      "          14       0.02      0.02      0.02        52\n",
      "          15       0.03      0.04      0.03        28\n",
      "          16       0.13      0.16      0.14       135\n",
      "          17       0.40      0.35      0.37        23\n",
      "          18       0.31      0.44      0.36        25\n",
      "          19       0.23      0.26      0.24        50\n",
      "          20       0.22      0.24      0.23        29\n",
      "          21       0.71      0.70      0.70       164\n",
      "          22       0.61      0.42      0.50        45\n",
      "          23       0.28      0.23      0.26        77\n",
      "          24       0.47      0.44      0.45        32\n",
      "          25       0.16      0.28      0.21        47\n",
      "          26       0.02      0.04      0.03        25\n",
      "          27       0.32      0.39      0.35       218\n",
      "          28       0.30      0.33      0.32        48\n",
      "          29       0.12      0.15      0.13        39\n",
      "          30       0.41      0.39      0.40        38\n",
      "          31       0.12      0.08      0.10        38\n",
      "          32       0.14      0.15      0.14        53\n",
      "          33       0.31      0.28      0.29        57\n",
      "          34       0.50      0.53      0.51        57\n",
      "          35       0.09      0.10      0.09       102\n",
      "          36       0.24      0.26      0.25        90\n",
      "          37       0.05      0.04      0.04        23\n",
      "          38       0.44      0.36      0.40       136\n",
      "          39       0.12      0.13      0.12        45\n",
      "          40       0.41      0.42      0.41       243\n",
      "          41       0.32      0.31      0.31        49\n",
      "          42       0.30      0.39      0.34       113\n",
      "          43       0.31      0.29      0.30       133\n",
      "          44       0.14      0.18      0.15       118\n",
      "          45       0.12      0.16      0.14        31\n",
      "          46       0.21      0.19      0.20       105\n",
      "          47       0.19      0.14      0.16        29\n",
      "          48       0.16      0.14      0.15        29\n",
      "          49       0.25      0.24      0.24        68\n",
      "          50       0.02      0.02      0.02        47\n",
      "          51       0.02      0.05      0.03        22\n",
      "          52       0.40      0.22      0.29        36\n",
      "          53       0.38      0.34      0.36        29\n",
      "          54       0.57      0.54      0.56       116\n",
      "          55       0.66      0.59      0.62        99\n",
      "          56       0.36      0.30      0.33        46\n",
      "          57       0.49      0.47      0.48        38\n",
      "          58       0.51      0.43      0.47        53\n",
      "          59       0.17      0.19      0.18       160\n",
      "\n",
      "   micro avg       0.28      0.30      0.29      3991\n",
      "   macro avg       0.27      0.27      0.27      3991\n",
      "weighted avg       0.30      0.30      0.29      3991\n",
      " samples avg       0.25      0.30      0.25      3991\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\decique\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print('Metrics for Bag-of-Words:')\n",
    "print(classification_report(y_val, y_pred_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate `recall@k`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@k for Bag-of-Words: 0.4194\n"
     ]
    }
   ],
   "source": [
    "bow_recallk = recallk(df_val.tags, df_val.predicted_tags)\n",
    "print(f'Recall@k for Bag-of-Words: {bow_recallk:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Training with IF-IDF embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\decique\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train = [' '.join(txt) for txt in data_train.text_markdown]\n",
    "X_val = [' '.join(txt) for txt in data_val.text_markdown]\n",
    "X_test = [' '.join(txt) for txt in data_test.text_markdown]\n",
    "\n",
    "Tfidf_Vec = TfidfVectorizer(tokenizer = lambda x: x.split())\n",
    "\n",
    "Tfidf_Vec.fit(X_train)\n",
    "X_train = Tfidf_Vec.transform(X_train)\n",
    "X_test = Tfidf_Vec.transform(X_test)\n",
    "X_val = Tfidf_Vec.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X TF-IDF's shapes:\n",
      "   • X train shape: (25209, 5899)\n",
      "   • X val shape: (2821, 5899)\n",
      "   • X test shape: (3119, 5899)\n"
     ]
    }
   ],
   "source": [
    "print(\"X TF-IDF's shapes:\")\n",
    "print(f'   • X train shape: {X_train.shape}')\n",
    "print(f'   • X val shape: {X_val.shape}')\n",
    "print(f'   • X test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg_cfg = {'C':5e7,\n",
    "              'penalty': 'l2',\n",
    "              'dual': False,\n",
    "              'class_weight': tag_distr_formated,\n",
    "              'solver': 'liblinear',\n",
    "              'random_state': 42}\n",
    "\n",
    "clf_ovr = OneVsRestClassifier(estimator=LogisticRegression(C=LogReg_cfg['C'],\n",
    "                                                           dual=LogReg_cfg['dual'],\n",
    "                                                           class_weight=LogReg_cfg['class_weight'],\n",
    "                                                           penalty=LogReg_cfg['penalty'],\n",
    "                                                           solver=LogReg_cfg['solver'],\n",
    "                                                           random_state=LogReg_cfg['random_state']),\n",
    "                              n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(save_models_path + 'tf_idf.joblib'):\n",
    "    clf_ovr = load(save_models_path + 'tf_idf.joblib')\n",
    "else:\n",
    "    clf_ovr.fit(X_train, y_train)\n",
    "    dump(clf_ovr, save_models_path + 'tf_idf.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val = clf_ovr.predict(X_val)\n",
    "\n",
    "df_val = data_val.copy()\n",
    "df_val['predicted_tags'] = Vec.inverse_transform(y_pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.24      0.31        25\n",
      "           1       0.32      0.24      0.28        45\n",
      "           2       0.68      0.33      0.44        40\n",
      "           3       0.41      0.26      0.32        27\n",
      "           4       0.55      0.44      0.49        27\n",
      "           5       0.12      0.13      0.13        68\n",
      "           6       0.52      0.28      0.36        40\n",
      "           7       0.28      0.25      0.27        59\n",
      "           8       0.09      0.05      0.06        42\n",
      "           9       0.31      0.35      0.33       113\n",
      "          10       0.33      0.20      0.25        50\n",
      "          11       0.03      0.05      0.04        85\n",
      "          12       0.10      0.14      0.12        72\n",
      "          13       0.63      0.47      0.53        58\n",
      "          14       0.03      0.02      0.02        52\n",
      "          15       0.00      0.00      0.00        28\n",
      "          16       0.13      0.17      0.15       135\n",
      "          17       0.64      0.30      0.41        23\n",
      "          18       0.56      0.40      0.47        25\n",
      "          19       0.34      0.28      0.31        50\n",
      "          20       0.33      0.21      0.26        29\n",
      "          21       0.71      0.71      0.71       164\n",
      "          22       0.79      0.42      0.55        45\n",
      "          23       0.35      0.27      0.31        77\n",
      "          24       0.38      0.31      0.34        32\n",
      "          25       0.18      0.19      0.19        47\n",
      "          26       0.05      0.04      0.05        25\n",
      "          27       0.29      0.38      0.33       218\n",
      "          28       0.38      0.25      0.30        48\n",
      "          29       0.20      0.13      0.16        39\n",
      "          30       0.58      0.39      0.47        38\n",
      "          31       0.27      0.08      0.12        38\n",
      "          32       0.19      0.15      0.17        53\n",
      "          33       0.26      0.21      0.23        57\n",
      "          34       0.62      0.49      0.55        57\n",
      "          35       0.12      0.16      0.14       102\n",
      "          36       0.31      0.28      0.29        90\n",
      "          37       0.12      0.04      0.06        23\n",
      "          38       0.39      0.38      0.39       136\n",
      "          39       0.13      0.13      0.13        45\n",
      "          40       0.32      0.40      0.35       243\n",
      "          41       0.42      0.22      0.29        49\n",
      "          42       0.36      0.41      0.38       113\n",
      "          43       0.32      0.29      0.31       133\n",
      "          44       0.13      0.20      0.16       118\n",
      "          45       0.17      0.10      0.12        31\n",
      "          46       0.16      0.19      0.17       105\n",
      "          47       0.33      0.14      0.20        29\n",
      "          48       0.35      0.24      0.29        29\n",
      "          49       0.33      0.26      0.29        68\n",
      "          50       0.00      0.00      0.00        47\n",
      "          51       0.00      0.00      0.00        22\n",
      "          52       0.62      0.22      0.33        36\n",
      "          53       0.41      0.31      0.35        29\n",
      "          54       0.63      0.53      0.58       116\n",
      "          55       0.66      0.61      0.63        99\n",
      "          56       0.59      0.28      0.38        46\n",
      "          57       0.54      0.39      0.45        38\n",
      "          58       0.56      0.42      0.48        53\n",
      "          59       0.14      0.19      0.16       160\n",
      "\n",
      "   micro avg       0.31      0.29      0.30      3991\n",
      "   macro avg       0.34      0.25      0.28      3991\n",
      "weighted avg       0.33      0.29      0.30      3991\n",
      " samples avg       0.25      0.30      0.25      3991\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Metrics for TF-IDF:')\n",
    "print(classification_report(y_val, y_pred_val, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate `recall@k`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@k on TF-IDF: 0.4133\n"
     ]
    }
   ],
   "source": [
    "tf_idf_recallk = recallk(df_val.tags, df_val.predicted_tags)\n",
    "print(f'Recall@k on TF-IDF: {tf_idf_recallk:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Training on TF-IDF with N-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\decique\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train = [' '.join(txt) for txt in data_train.text_markdown]\n",
    "X_val = [' '.join(txt) for txt in data_val.text_markdown]\n",
    "X_test = [' '.join(txt) for txt in data_test.text_markdown]\n",
    "\n",
    "Tfidf_Vec = TfidfVectorizer(tokenizer = lambda x: x.split(), ngram_range=(1, 2))\n",
    "\n",
    "Tfidf_Vec.fit(X_train)\n",
    "X_train = Tfidf_Vec.transform(X_train)\n",
    "X_test = Tfidf_Vec.transform(X_test)\n",
    "X_val = Tfidf_Vec.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X TF-IDF with n-grams's shapes:\n",
      "   • X train shape: (25209, 1654803)\n",
      "   • X val shape: (2821, 1654803)\n",
      "   • X test shape: (3119, 1654803)\n"
     ]
    }
   ],
   "source": [
    "print(\"X TF-IDF with n-grams's shapes:\")\n",
    "print(f'   • X train shape: {X_train.shape}')\n",
    "print(f'   • X val shape: {X_val.shape}')\n",
    "print(f'   • X test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg_cfg = {'C':1e8,\n",
    "              'penalty': 'l2',\n",
    "              'dual': False,\n",
    "              'class_weight': tag_distr_formated,\n",
    "              'solver': 'lbfgs',\n",
    "              'random_state': 42}\n",
    "\n",
    "clf_ovr = OneVsRestClassifier(estimator=LogisticRegression(C=LogReg_cfg['C'],\n",
    "                                                           dual=LogReg_cfg['dual'],\n",
    "                                                           class_weight=LogReg_cfg['class_weight'],\n",
    "                                                           penalty=LogReg_cfg['penalty'],\n",
    "                                                           solver=LogReg_cfg['solver'],\n",
    "                                                           random_state=LogReg_cfg['random_state']),\n",
    "                              n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(save_models_path + 'tf_idf_ngrams.joblib'):\n",
    "    clf_ovr = load(save_models_path + 'tf_idf_ngrams.joblib')\n",
    "else:\n",
    "    clf_ovr.fit(X_train, y_train)\n",
    "    dump(clf_ovr, save_models_path + 'tf_idf_ngrams.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val = clf_ovr.predict(X_val)\n",
    "\n",
    "df_val = data_val.copy()\n",
    "df_val['predicted_tags'] = Vec.inverse_transform(y_pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for TF-IDF with 1-2 n-grams:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.28      0.35        25\n",
      "           1       0.36      0.27      0.31        45\n",
      "           2       0.58      0.28      0.37        40\n",
      "           3       0.71      0.19      0.29        27\n",
      "           4       0.60      0.44      0.51        27\n",
      "           5       0.39      0.10      0.16        68\n",
      "           6       0.45      0.25      0.32        40\n",
      "           7       0.50      0.19      0.27        59\n",
      "           8       0.11      0.02      0.04        42\n",
      "           9       0.47      0.32      0.38       113\n",
      "          10       0.46      0.32      0.38        50\n",
      "          11       0.17      0.01      0.02        85\n",
      "          12       0.22      0.06      0.09        72\n",
      "          13       0.68      0.52      0.59        58\n",
      "          14       0.25      0.02      0.04        52\n",
      "          15       0.00      0.00      0.00        28\n",
      "          16       0.34      0.10      0.16       135\n",
      "          17       0.79      0.48      0.59        23\n",
      "          18       0.57      0.52      0.54        25\n",
      "          19       0.41      0.26      0.32        50\n",
      "          20       0.32      0.21      0.25        29\n",
      "          21       0.80      0.71      0.75       164\n",
      "          22       0.75      0.40      0.52        45\n",
      "          23       0.53      0.25      0.34        77\n",
      "          24       0.57      0.38      0.45        32\n",
      "          25       0.21      0.13      0.16        47\n",
      "          26       0.00      0.00      0.00        25\n",
      "          27       0.49      0.39      0.44       218\n",
      "          28       0.41      0.19      0.26        48\n",
      "          29       0.28      0.13      0.18        39\n",
      "          30       0.70      0.50      0.58        38\n",
      "          31       0.40      0.05      0.09        38\n",
      "          32       0.45      0.19      0.27        53\n",
      "          33       0.49      0.37      0.42        57\n",
      "          34       0.70      0.49      0.58        57\n",
      "          35       0.46      0.06      0.10       102\n",
      "          36       0.63      0.27      0.38        90\n",
      "          37       0.17      0.04      0.07        23\n",
      "          38       0.63      0.41      0.50       136\n",
      "          39       0.18      0.13      0.15        45\n",
      "          40       0.64      0.40      0.49       243\n",
      "          41       0.45      0.27      0.33        49\n",
      "          42       0.60      0.35      0.44       113\n",
      "          43       0.48      0.32      0.38       133\n",
      "          44       0.24      0.18      0.20       118\n",
      "          45       0.20      0.13      0.16        31\n",
      "          46       0.41      0.17      0.24       105\n",
      "          47       0.29      0.07      0.11        29\n",
      "          48       0.50      0.24      0.33        29\n",
      "          49       0.42      0.21      0.28        68\n",
      "          50       0.20      0.04      0.07        47\n",
      "          51       0.10      0.05      0.06        22\n",
      "          52       0.56      0.25      0.35        36\n",
      "          53       0.48      0.45      0.46        29\n",
      "          54       0.75      0.39      0.51       116\n",
      "          55       0.75      0.62      0.68        99\n",
      "          56       0.53      0.35      0.42        46\n",
      "          57       0.56      0.58      0.57        38\n",
      "          58       0.71      0.57      0.63        53\n",
      "          59       0.23      0.07      0.11       160\n",
      "\n",
      "   micro avg       0.52      0.28      0.37      3991\n",
      "   macro avg       0.45      0.26      0.32      3991\n",
      "weighted avg       0.48      0.28      0.34      3991\n",
      " samples avg       0.30      0.29      0.28      3991\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Metrics for TF-IDF with 1-2 n-grams:')\n",
    "print(classification_report(y_val, y_pred_val, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate `recall@k`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@k for TF-IDF with 1-2 n-grams: 0.4016\n"
     ]
    }
   ],
   "source": [
    "n_gram_tf_idf_recallk = recallk(df_val.tags, df_val.predicted_tags)\n",
    "print(f'Recall@k for TF-IDF with 1-2 n-grams: {n_gram_tf_idf_recallk:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Training on rubert-tiny-v2 embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_paths = module_path + '/data/embeddings/rubert-tiny-v2/'\n",
    "\n",
    "emb_pth = [emb_paths + 'texts.parquet']\n",
    "emb = merge_dataset(emb_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2936217</td>\n",
       "      <td>[0.3411005, -0.16877297, -0.3599054, 0.011505239, -0.19693527, 0.16206133, -0.62560713, -0.38459125, -0.08364315, -0.17384137, -0.2905479, 0.8844394, -0.07451144, 1.9678769, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6991412</td>\n",
       "      <td>[0.3696494, 0.06409113, -0.62138826, -0.8906186, 0.08984075, 0.27482352, -0.31647494, -0.778525, -0.6068895, 0.42193377, -0.05778958, 0.017193496, 0.14765958, 0.35776424, -0.16...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  \\\n",
       "0  2936217   \n",
       "1  6991412   \n",
       "\n",
       "                                                                                                                                                                             embedding  \n",
       "0  [0.3411005, -0.16877297, -0.3599054, 0.011505239, -0.19693527, 0.16206133, -0.62560713, -0.38459125, -0.08364315, -0.17384137, -0.2905479, 0.8844394, -0.07451144, 1.9678769, 0....  \n",
       "1  [0.3696494, 0.06409113, -0.62138826, -0.8906186, 0.08984075, 0.27482352, -0.31647494, -0.778525, -0.6068895, 0.42193377, -0.05778958, 0.017193496, 0.14765958, 0.35776424, -0.16...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = emb[emb['id'].isin(data['id'])]\n",
    "\n",
    "emb_train = emb[emb['id'].isin(data_train['id'])]\n",
    "emb_val = emb[emb['id'].isin(data_val['id'])]\n",
    "emb_test = emb[emb['id'].isin(data_test['id'])]\n",
    "\n",
    "assert len(emb_train) == len(data_train), \"Something went wrong!\"\n",
    "assert len(emb_val) == len(data_val), \"Something went wrong!\"\n",
    "assert len(emb_test) == len(data_test), \"Something went wrong!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_emb = [i for i in emb_train.embedding]\n",
    "X_val_emb = [i for i in emb_val.embedding]\n",
    "X_test_emb = [i for i in emb_test.embedding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X embeddings shapes:\n",
      "   • X train shape: (25209, 312)\n",
      "   • X val shape: (2821, 312)\n",
      "   • X test shape: (3119, 312)\n"
     ]
    }
   ],
   "source": [
    "print(\"X embeddings shapes:\")\n",
    "print(f'   • X train shape: {np.shape(X_train_emb)}')\n",
    "print(f'   • X val shape: {np.shape(X_val_emb)}')\n",
    "print(f'   • X test shape: {np.shape(X_test_emb)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg_cfg = {'C':1e8,\n",
    "              'penalty': 'l2',\n",
    "              'dual': False,\n",
    "              'class_weight': tag_distr_formated,\n",
    "              'solver': 'lbfgs',\n",
    "              'random_state': 42}\n",
    "\n",
    "clf_ovr = OneVsRestClassifier(estimator=LogisticRegression(C=LogReg_cfg['C'],\n",
    "                                                           dual=LogReg_cfg['dual'],\n",
    "                                                           class_weight=LogReg_cfg['class_weight'],\n",
    "                                                           penalty=LogReg_cfg['penalty'],\n",
    "                                                           solver=LogReg_cfg['solver'],\n",
    "                                                           random_state=LogReg_cfg['random_state']),\n",
    "                              n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(save_models_path + 'rubert.joblib'):\n",
    "    clf_ovr = load(save_models_path + 'rubert.joblib')\n",
    "else:\n",
    "    clf_ovr.fit(X_train_emb, y_train)\n",
    "    dump(clf_ovr, save_models_path + 'rubert.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val = clf_ovr.predict(X_val_emb)\n",
    "\n",
    "df_val = data_val.copy()\n",
    "df_val['predicted_tags'] = Vec.inverse_transform(y_pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for rubert embeddings:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.32      0.31        25\n",
      "           1       0.39      0.31      0.35        45\n",
      "           2       0.58      0.17      0.27        40\n",
      "           3       0.47      0.33      0.39        27\n",
      "           4       0.35      0.30      0.32        27\n",
      "           5       0.16      0.04      0.07        68\n",
      "           6       0.29      0.15      0.20        40\n",
      "           7       0.36      0.20      0.26        59\n",
      "           8       0.35      0.17      0.23        42\n",
      "           9       0.42      0.34      0.37       113\n",
      "          10       0.30      0.22      0.25        50\n",
      "          11       0.10      0.01      0.02        85\n",
      "          12       0.25      0.01      0.03        72\n",
      "          13       0.63      0.59      0.61        58\n",
      "          14       0.00      0.00      0.00        52\n",
      "          15       0.00      0.00      0.00        28\n",
      "          16       0.25      0.03      0.05       135\n",
      "          17       0.58      0.61      0.60        23\n",
      "          18       0.55      0.64      0.59        25\n",
      "          19       0.28      0.20      0.23        50\n",
      "          20       0.24      0.21      0.22        29\n",
      "          21       0.69      0.64      0.66       164\n",
      "          22       0.72      0.51      0.60        45\n",
      "          23       0.32      0.09      0.14        77\n",
      "          24       0.46      0.34      0.39        32\n",
      "          25       0.25      0.13      0.17        47\n",
      "          26       0.00      0.00      0.00        25\n",
      "          27       0.49      0.24      0.32       218\n",
      "          28       0.30      0.27      0.28        48\n",
      "          29       0.42      0.13      0.20        39\n",
      "          30       0.49      0.47      0.48        38\n",
      "          31       0.12      0.03      0.04        38\n",
      "          32       0.60      0.11      0.19        53\n",
      "          33       0.37      0.30      0.33        57\n",
      "          34       0.55      0.40      0.46        57\n",
      "          35       0.08      0.01      0.02       102\n",
      "          36       0.61      0.22      0.33        90\n",
      "          37       0.04      0.04      0.04        23\n",
      "          38       0.64      0.43      0.51       136\n",
      "          39       0.27      0.29      0.28        45\n",
      "          40       0.57      0.42      0.48       243\n",
      "          41       0.42      0.27      0.33        49\n",
      "          42       0.56      0.39      0.46       113\n",
      "          43       0.47      0.27      0.34       133\n",
      "          44       0.27      0.08      0.12       118\n",
      "          45       0.12      0.06      0.08        31\n",
      "          46       0.39      0.21      0.27       105\n",
      "          47       0.36      0.14      0.20        29\n",
      "          48       0.17      0.07      0.10        29\n",
      "          49       0.33      0.16      0.22        68\n",
      "          50       0.17      0.02      0.04        47\n",
      "          51       0.12      0.05      0.07        22\n",
      "          52       0.36      0.22      0.28        36\n",
      "          53       0.38      0.31      0.34        29\n",
      "          54       0.72      0.72      0.72       116\n",
      "          55       0.75      0.60      0.66        99\n",
      "          56       0.50      0.41      0.45        46\n",
      "          57       0.46      0.42      0.44        38\n",
      "          58       0.53      0.47      0.50        53\n",
      "          59       0.27      0.04      0.07       160\n",
      "\n",
      "   micro avg       0.48      0.27      0.34      3991\n",
      "   macro avg       0.37      0.25      0.28      3991\n",
      "weighted avg       0.41      0.27      0.31      3991\n",
      " samples avg       0.28      0.28      0.26      3991\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Metrics for rubert embeddings:')\n",
    "print(classification_report(y_val, y_pred_val, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate `recall@k`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@k for model with rubert embeddings: 0.3761\n"
     ]
    }
   ],
   "source": [
    "rubert_recallk = recallk(df_val.tags, df_val.predicted_tags)\n",
    "print(f'Recall@k for model with rubert embeddings: {rubert_recallk:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Results\n",
    "\n",
    "As the result of training with different embeddings, we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@k's for models:\n",
      "\n",
      "Recall@k for Bag-of-Words: 0.419355\n",
      "Recall@k for TF-IDF: 0.413329\n",
      "Recall@k for TF-IDF with uni- and bi- grams: 0.401631\n",
      "Recall@k for rubert embeddings: 0.376108\n"
     ]
    }
   ],
   "source": [
    "print(\"Recall@k's for models:\\n\")\n",
    "print(f'Recall@k for Bag-of-Words: {bow_recallk:3f}')\n",
    "print(f'Recall@k for TF-IDF: {tf_idf_recallk:3f}')\n",
    "print(f'Recall@k for TF-IDF with uni- and bi- grams: {n_gram_tf_idf_recallk:3f}')\n",
    "print(f'Recall@k for rubert embeddings: {rubert_recallk:3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
