_target_: pytorch_models.models.rate_module.RateModule

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 1e-4
  weight_decay: 0.0

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.1
  patience: 10

net:
  _target_: pytorch_models.models.components.transformer.RegressionTransformer
  model_path: cointegrated/rubert-tiny2
  input_dim: 312 # depends on encoder output dim, 768 for bert-base
  hidden_dim: 64
  output_size: 2
  dropout_rate: 0.2
  freeze: False
  use_sigmoid: False
# compile model for faster training with pytorch 2.0
compile: false
freeze_after: null